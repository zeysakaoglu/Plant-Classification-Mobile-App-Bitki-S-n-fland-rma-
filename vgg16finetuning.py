# -*- coding: utf-8 -*-
"""VGG16FineTuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uHIa0y4fRD9dYUD0nfWkysCLm87uXY0A
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip -o -q "/content/drive/MyDrive/Dataset-1.zip" -d /content/

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Model
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# 4. Veri yollarÄ± ve generatorlar
train_dir = '/content/Dataset-1/train'
val_dir = '/content/Dataset-1/val'
test_dir = '/content/Dataset-1/test'

datagen = ImageDataGenerator(rescale=1./255)

train_gen = datagen.flow_from_directory(train_dir, target_size=(224, 224), batch_size=32, class_mode='categorical')
val_gen = datagen.flow_from_directory(val_dir, target_size=(224, 224), batch_size=32, class_mode='categorical')
test_gen = datagen.flow_from_directory(test_dir, target_size=(224, 224), batch_size=32, class_mode='categorical', shuffle=False)

base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
for layer in base_model.layers:
    layer.trainable = False  # Hepsi dondu
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.5)(x)
output = Dense(train_gen.num_classes, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=output)

model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])

history_tl = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=50
)

# Son 4 katmanÄ± aÃ§
for layer in base_model.layers[-10:]:
    layer.trainable = True


model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])


history_ft = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=5
)

y_probs = model.predict(test_gen)
y_pred = np.argmax(y_probs, axis=1)
y_true = test_gen.classes
labels = list(test_gen.class_indices.keys())


print("ðŸ“Š VGG16 (Transfer + Fine-Tuning) SonuÃ§larÄ±:")
print(classification_report(y_true, y_pred, target_names=labels))


cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=labels, yticklabels=labels)
plt.title("VGG16 â€“ Confusion Matrix (Fine-Tuned)")
plt.xlabel("Tahmin")
plt.ylabel("GerÃ§ek")
plt.show()

acc = history_tl.history['accuracy'] + history_ft.history['accuracy']
val_acc = history_tl.history['val_accuracy'] + history_ft.history['val_accuracy']
loss = history_tl.history['loss'] + history_ft.history['loss']
val_loss = history_tl.history['val_loss'] + history_ft.history['val_loss']
epochs_range = range(len(acc))

# Accuracy GrafiÄŸi
plt.figure(figsize=(8, 5))
plt.plot(epochs_range, acc, label='Train Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.title('Model Accuracy (Transfer + Fine-Tuning)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Loss GrafiÄŸi
plt.figure(figsize=(8, 5))
plt.plot(epochs_range, loss, label='Train Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.title('Model Loss (Transfer + Fine-Tuning)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

model.save('/content/drive/MyDrive/Modeller/vgg16finetuning.h5')